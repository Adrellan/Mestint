{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "***************************************************************\n",
      "Failed to import TensorFlow. Please note that TensorFlow is not installed by default when you install TFDS. This allows you to choose to install either `tf-nightly` or `tensorflow`. Please install the most recent version of TensorFlow, by following instructions at https://tensorflow.org/install.\n",
      "***************************************************************\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "Failed to construct dataset imdb_reviews: No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow_datasets\\core\\utils\\gcs_utils.py:76\u001b[0m, in \u001b[0;36mexists\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 76\u001b[0m   \u001b[39mreturn\u001b[39;00m path\u001b[39m.\u001b[39;49mexists()\n\u001b[0;32m     77\u001b[0m \u001b[39mexcept\u001b[39;00m gcs_unavailable_exceptions():  \u001b[39m# pylint: disable=catching-non-exception\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\etils\\epath\\gpath.py:130\u001b[0m, in \u001b[0;36m_GPath.exists\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Returns True if self exists.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 130\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mexists(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_path_str)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\etils\\epath\\backend.py:204\u001b[0m, in \u001b[0;36m_TfBackend.exists\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexists\u001b[39m(\u001b[39mself\u001b[39m, path: PathLike) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgfile\u001b[39m.\u001b[39mexists(path)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\etils\\epath\\backend.py:194\u001b[0m, in \u001b[0;36m_TfBackend.gfile\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[0;32m    193\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgfile\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 194\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtf\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mgfile\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\etils\\epath\\backend.py:187\u001b[0m, in \u001b[0;36m_TfBackend.tf\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m--> 187\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[0;32m    188\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m. To use epath.Path with gs://, TensorFlow should be installed.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    189\u001b[0m   ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    190\u001b[0m \u001b[39mreturn\u001b[39;00m tensorflow\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'tensorflow'. To use epath.Path with gs://, TensorFlow should be installed.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow_datasets\\core\\utils\\py_utils.py:415\u001b[0m, in \u001b[0;36mtry_reraise\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    414\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 415\u001b[0m   \u001b[39myield\u001b[39;00m\n\u001b[0;32m    416\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow_datasets\\core\\load.py:212\u001b[0m, in \u001b[0;36mbuilder\u001b[1;34m(name, try_gcs, **builder_kwargs)\u001b[0m\n\u001b[0;32m    211\u001b[0m   \u001b[39mwith\u001b[39;00m py_utils\u001b[39m.\u001b[39mtry_reraise(prefix\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFailed to construct dataset \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m--> 212\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mbuilder_kwargs)  \u001b[39m# pytype: disable=not-instantiable\u001b[39;00m\n\u001b[0;32m    214\u001b[0m \u001b[39m# If neither the code nor the files are found, raise DatasetNotFoundError\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow_datasets\\core\\logging\\__init__.py:288\u001b[0m, in \u001b[0;36mbuilder_init.<locals>.decorator\u001b[1;34m(function, dsbuilder, args, kwargs)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 288\u001b[0m   \u001b[39mreturn\u001b[39;00m function(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    289\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:1270\u001b[0m, in \u001b[0;36mFileReaderBuilder.__init__\u001b[1;34m(self, file_format, **kwargs)\u001b[0m\n\u001b[0;32m   1260\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Initializes an instance of FileReaderBuilder.\u001b[39;00m\n\u001b[0;32m   1261\u001b[0m \n\u001b[0;32m   1262\u001b[0m \u001b[39mCallers must pass arguments as keyword arguments.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1268\u001b[0m \u001b[39m  **kwargs: Arguments passed to `DatasetBuilder`.\u001b[39;00m\n\u001b[0;32m   1269\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1270\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1271\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mset_file_format(file_format)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow_datasets\\core\\logging\\__init__.py:288\u001b[0m, in \u001b[0;36mbuilder_init.<locals>.decorator\u001b[1;34m(function, dsbuilder, args, kwargs)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 288\u001b[0m   \u001b[39mreturn\u001b[39;00m function(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    289\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:275\u001b[0m, in \u001b[0;36mDatasetBuilder.__init__\u001b[1;34m(self, data_dir, config, version)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# Use the code version (do not restore data)\u001b[39;00m\n\u001b[1;32m--> 275\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minfo\u001b[39m.\u001b[39;49minitialize_from_bucket()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow_datasets\\core\\dataset_info.py:733\u001b[0m, in \u001b[0;36mDatasetInfo.initialize_from_bucket\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    732\u001b[0m tmp_dir \u001b[39m=\u001b[39m epath\u001b[39m.\u001b[39mPath(tempfile\u001b[39m.\u001b[39mmkdtemp(\u001b[39m\"\u001b[39m\u001b[39mtfds\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m--> 733\u001b[0m data_files \u001b[39m=\u001b[39m gcs_utils\u001b[39m.\u001b[39;49mgcs_dataset_info_files(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfull_name)\n\u001b[0;32m    734\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m data_files:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow_datasets\\core\\utils\\gcs_utils.py:100\u001b[0m, in \u001b[0;36mgcs_dataset_info_files\u001b[1;34m(dataset_name)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Return paths to the dataset info files of the given dataset in gs://tfds-data.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 100\u001b[0m path \u001b[39m=\u001b[39m gcs_dataset_info_path(dataset_name)\n\u001b[0;32m    101\u001b[0m \u001b[39mif\u001b[39;00m path \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow_datasets\\core\\utils\\gcs_utils.py:93\u001b[0m, in \u001b[0;36mgcs_dataset_info_path\u001b[1;34m(dataset_name)\u001b[0m\n\u001b[0;32m     92\u001b[0m path \u001b[39m=\u001b[39m gcs_path(posixpath\u001b[39m.\u001b[39mjoin(GCS_DATASET_INFO_DIR, dataset_name))\n\u001b[1;32m---> 93\u001b[0m \u001b[39mif\u001b[39;00m _is_gcs_disabled \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m exists(path):\n\u001b[0;32m     94\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow_datasets\\core\\utils\\gcs_utils.py:77\u001b[0m, in \u001b[0;36mexists\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     76\u001b[0m   \u001b[39mreturn\u001b[39;00m path\u001b[39m.\u001b[39mexists()\n\u001b[1;32m---> 77\u001b[0m \u001b[39mexcept\u001b[39;00m gcs_unavailable_exceptions():  \u001b[39m# pylint: disable=catching-non-exception\u001b[39;00m\n\u001b[0;32m     78\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow_datasets\\core\\utils\\gcs_utils.py:46\u001b[0m, in \u001b[0;36mgcs_unavailable_exceptions\u001b[1;34m()\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgcs_unavailable_exceptions\u001b[39m():\n\u001b[0;32m     44\u001b[0m   \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m     45\u001b[0m       \u001b[39mOSError\u001b[39;00m,\n\u001b[1;32m---> 46\u001b[0m       tf\u001b[39m.\u001b[39;49merrors\u001b[39m.\u001b[39mUnimplementedError,\n\u001b[0;32m     47\u001b[0m       tf\u001b[39m.\u001b[39merrors\u001b[39m.\u001b[39mFailedPreconditionError,\n\u001b[0;32m     48\u001b[0m       tf\u001b[39m.\u001b[39merrors\u001b[39m.\u001b[39mPermissionDeniedError,\n\u001b[0;32m     49\u001b[0m       tf\u001b[39m.\u001b[39merrors\u001b[39m.\u001b[39mAbortedError,\n\u001b[0;32m     50\u001b[0m   )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow_datasets\\core\\utils\\lazy_imports_utils.py:85\u001b[0m, in \u001b[0;36mLazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m     84\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39merror_callback(exception\u001b[39m=\u001b[39mexception, module_name\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule_name)\n\u001b[1;32m---> 85\u001b[0m     \u001b[39mraise\u001b[39;00m exception\n\u001b[0;32m     86\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule, name)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow_datasets\\core\\utils\\lazy_imports_utils.py:74\u001b[0m, in \u001b[0;36mLazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m     73\u001b[0m start_import_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter()\n\u001b[1;32m---> 74\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39;49mimport_module(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodule_name)\n\u001b[0;32m     75\u001b[0m import_time_ms \u001b[39m=\u001b[39m (time\u001b[39m.\u001b[39mperf_counter() \u001b[39m-\u001b[39m start_import_time) \u001b[39m*\u001b[39m \u001b[39m1000\u001b[39m\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1206\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1178\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1142\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtfds\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtqdm\u001b[39;00m \u001b[39mimport\u001b[39;00m tqdm\n\u001b[1;32m----> 5\u001b[0m dataset_train \u001b[39m=\u001b[39m tfds\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39mimdb_reviews\u001b[39;49m\u001b[39m'\u001b[39;49m, split\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m, shuffle_files\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m      6\u001b[0m dataset_test \u001b[39m=\u001b[39m tfds\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39mimdb_reviews\u001b[39m\u001b[39m'\u001b[39m, split\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m, shuffle_files\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow_datasets\\core\\logging\\__init__.py:169\u001b[0m, in \u001b[0;36m_FunctionDecorator.__call__\u001b[1;34m(self, function, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    167\u001b[0m metadata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_call()\n\u001b[0;32m    168\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m   \u001b[39mreturn\u001b[39;00m function(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    170\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m    171\u001b[0m   metadata\u001b[39m.\u001b[39mmark_error()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow_datasets\\core\\load.py:634\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001b[0m\n\u001b[0;32m    502\u001b[0m \u001b[39m@tfds_logging\u001b[39m\u001b[39m.\u001b[39mload()\n\u001b[0;32m    503\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\n\u001b[0;32m    504\u001b[0m     name: \u001b[39mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m ):\n\u001b[0;32m    520\u001b[0m   \u001b[39m# pylint: disable=line-too-long\u001b[39;00m\n\u001b[0;32m    521\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Loads the named dataset into a `tf.data.Dataset`.\u001b[39;00m\n\u001b[0;32m    522\u001b[0m \n\u001b[0;32m    523\u001b[0m \u001b[39m  `tfds.load` is a convenience method that:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    632\u001b[0m \u001b[39m      Split-specific information is available in `ds_info.splits`.\u001b[39;00m\n\u001b[0;32m    633\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m   dbuilder \u001b[39m=\u001b[39m _fetch_builder(\n\u001b[0;32m    635\u001b[0m       name,\n\u001b[0;32m    636\u001b[0m       data_dir,\n\u001b[0;32m    637\u001b[0m       builder_kwargs,\n\u001b[0;32m    638\u001b[0m       try_gcs,\n\u001b[0;32m    639\u001b[0m   )\n\u001b[0;32m    640\u001b[0m   _download_and_prepare_builder(dbuilder, download, download_and_prepare_kwargs)\n\u001b[0;32m    642\u001b[0m   \u001b[39mif\u001b[39;00m as_dataset_kwargs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow_datasets\\core\\load.py:489\u001b[0m, in \u001b[0;36m_fetch_builder\u001b[1;34m(name, data_dir, builder_kwargs, try_gcs)\u001b[0m\n\u001b[0;32m    486\u001b[0m \u001b[39mif\u001b[39;00m builder_kwargs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    487\u001b[0m   builder_kwargs \u001b[39m=\u001b[39m {}\n\u001b[1;32m--> 489\u001b[0m \u001b[39mreturn\u001b[39;00m builder(name, data_dir\u001b[39m=\u001b[39;49mdata_dir, try_gcs\u001b[39m=\u001b[39;49mtry_gcs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mbuilder_kwargs)\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\contextlib.py:81\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[0;32m     79\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds):\n\u001b[0;32m     80\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_recreate_cm():\n\u001b[1;32m---> 81\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow_datasets\\core\\logging\\__init__.py:169\u001b[0m, in \u001b[0;36m_FunctionDecorator.__call__\u001b[1;34m(self, function, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    167\u001b[0m metadata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_call()\n\u001b[0;32m    168\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m   \u001b[39mreturn\u001b[39;00m function(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    170\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m    171\u001b[0m   metadata\u001b[39m.\u001b[39mmark_error()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow_datasets\\core\\load.py:211\u001b[0m, in \u001b[0;36mbuilder\u001b[1;34m(name, try_gcs, **builder_kwargs)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[39m# If code exists and loading from files was skipped (e.g. files not found),\u001b[39;00m\n\u001b[0;32m    209\u001b[0m \u001b[39m# load from the source code.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m:\n\u001b[1;32m--> 211\u001b[0m   \u001b[39mwith\u001b[39;49;00m py_utils\u001b[39m.\u001b[39;49mtry_reraise(prefix\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mFailed to construct dataset \u001b[39;49m\u001b[39m{\u001b[39;49;00mname\u001b[39m}\u001b[39;49;00m\u001b[39m: \u001b[39;49m\u001b[39m'\u001b[39;49m):\n\u001b[0;32m    212\u001b[0m     \u001b[39mreturn\u001b[39;49;00m \u001b[39mcls\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mbuilder_kwargs)  \u001b[39m# pytype: disable=not-instantiable\u001b[39;00m\n\u001b[0;32m    214\u001b[0m \u001b[39m# If neither the code nor the files are found, raise DatasetNotFoundError\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\contextlib.py:155\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[1;34m(self, typ, value, traceback)\u001b[0m\n\u001b[0;32m    153\u001b[0m     value \u001b[39m=\u001b[39m typ()\n\u001b[0;32m    154\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 155\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgen\u001b[39m.\u001b[39mthrow(typ, value, traceback)\n\u001b[0;32m    156\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[0;32m    157\u001b[0m     \u001b[39m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     \u001b[39m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[0;32m    159\u001b[0m     \u001b[39m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n\u001b[0;32m    160\u001b[0m     \u001b[39mreturn\u001b[39;00m exc \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m value\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow_datasets\\core\\utils\\py_utils.py:417\u001b[0m, in \u001b[0;36mtry_reraise\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    415\u001b[0m   \u001b[39myield\u001b[39;00m\n\u001b[0;32m    416\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m--> 417\u001b[0m   reraise(e, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow_datasets\\core\\utils\\py_utils.py:384\u001b[0m, in \u001b[0;36mreraise\u001b[1;34m(e, prefix, suffix)\u001b[0m\n\u001b[0;32m    382\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    383\u001b[0m     exception \u001b[39m=\u001b[39m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(e)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 384\u001b[0m   \u001b[39mraise\u001b[39;00m exception \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \u001b[39m# Otherwise, modify the exception in-place\u001b[39;00m\n\u001b[0;32m    386\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(e\u001b[39m.\u001b[39margs) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: Failed to construct dataset imdb_reviews: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow_datasets as tfds\n",
    "from tqdm import tqdm\n",
    "\n",
    "dataset_train = tfds.load('imdb_reviews', split='train', shuffle_files=True)\n",
    "dataset_test = tfds.load('imdb_reviews', split='test', shuffle_files=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m     data \u001b[39m=\u001b[39m [{ \u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m: item[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[0;32m      3\u001b[0m               \u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m: item[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mnumpy() } \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m tqdm(dataset)]\n\u001b[0;32m      4\u001b[0m     \u001b[39mreturn\u001b[39;00m pd\u001b[39m.\u001b[39mDataFrame(data)\n\u001b[1;32m----> 6\u001b[0m df_train \u001b[39m=\u001b[39m convert_to_df(dataset_train)\n\u001b[0;32m      7\u001b[0m df_test \u001b[39m=\u001b[39m convert_to_df(dataset_test)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset_train' is not defined"
     ]
    }
   ],
   "source": [
    "def convert_to_df(dataset):\n",
    "    data = [{ 'text': item['text'].numpy().decode('utf-8'),\n",
    "              'label': item['label'].numpy() } for item in tqdm(dataset)]\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "df_train = convert_to_df(dataset_train)\n",
    "df_test = convert_to_df(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m      5\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnegative\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> 7\u001b[0m def_train[\u001b[39m\"\u001b[39m\u001b[39msentiment\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m] \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(df_train)\n\u001b[0;32m      8\u001b[0m def_train[\u001b[39m\"\u001b[39m\u001b[39msentiment\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m df_train[\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mapply(sentiment)\n\u001b[0;32m      9\u001b[0m def_test[\u001b[39m\"\u001b[39m\u001b[39msentiment\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m] \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(df_test)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_train' is not defined"
     ]
    }
   ],
   "source": [
    "def sentiment(value):\n",
    "    if(value == 1):\n",
    "        return \"positive\"\n",
    "    else:\n",
    "        return \"negative\"\n",
    "\n",
    "def_train[\"sentiment\"] = [None] * len(df_train)\n",
    "def_train[\"sentiment\"] = df_train[\"label\"].apply(sentiment)\n",
    "def_test[\"sentiment\"] = [None] * len(df_test)\n",
    "def_test[\"sentiment\"] = df_test[\"label\"].apply(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hallgato\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "STOPWORDS = stopwords.words(\"english\")\n",
    "\n",
    "STOPWORDS[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "def remove_html_tags(text):\n",
    "    soup = BeautifulSoup(text,\"html.parser\")\n",
    "    text = soup.get_text()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_train[\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m df_train[\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mapply(remove_html_tags)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_train' is not defined"
     ]
    }
   ],
   "source": [
    "df_train[\"text\"] = df_train[\"text\"].apply(remove_html_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "tokenized_reviews = df_train[\"text\"].apply(lambda review_text:\n",
    "                                           word_tokenize(review_text.replace(\"\\n\",\"\").lower()))\n",
    "\n",
    "tokenized_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tqdm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m d \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m()\n\u001b[1;32m----> 3\u001b[0m \u001b[39mfor\u001b[39;00m review \u001b[39min\u001b[39;00m tqdm(tokenized_reviews):\n\u001b[0;32m      4\u001b[0m     \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m review:\n\u001b[0;32m      5\u001b[0m         \u001b[39mif\u001b[39;00m word \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m STOPWORDS \u001b[39mand\u001b[39;00m word\u001b[39m.\u001b[39misalpha():\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tqdm' is not defined"
     ]
    }
   ],
   "source": [
    "d = dict()\n",
    "\n",
    "for review in tqdm(tokenized_reviews):\n",
    "    for word in review:\n",
    "        if word not in STOPWORDS and word.isalpha():\n",
    "            d[word] = d.get(word, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = sorted(d.items(), key=lambda item: item[1], reverse=True)\n",
    "d[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m DESIRED_VOCAB_SIZE \u001b[39m=\u001b[39m \u001b[39m4000\u001b[39m\n\u001b[0;32m      3\u001b[0m VOCAB \u001b[39m=\u001b[39m [k \u001b[39mfor\u001b[39;00m k,v \u001b[39min\u001b[39;00m d[:DESIRED_VOCAB_SIZE]]\n\u001b[1;32m----> 4\u001b[0m word_table \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame({\u001b[39m\"\u001b[39m\u001b[39mword\u001b[39m\u001b[39m\"\u001b[39m: VOCAB})\n\u001b[0;32m      5\u001b[0m word_table\u001b[39m.\u001b[39mhead(\u001b[39m10\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "DESIRED_VOCAB_SIZE = 4000\n",
    "\n",
    "VOCAB = [k for k,v in d[:DESIRED_VOCAB_SIZE]]\n",
    "word_table = pd.DataFrame({\"word\": VOCAB})\n",
    "word_table.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_feqs = { \"positive\" : {}, \"negative\" : {}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_IDX = {}\n",
    "for i in range(0, len(word_table[\"word\"].values)):\n",
    "    VOCAB_IDX[word_table[\"word\"].values[i]] = i\n",
    "\n",
    "for idx in range(df_train.shape[0]):\n",
    "    review = df_train.iloc[idx][\"text\"]\n",
    "    sentiment = df_train.iloc[idx][\"sentiment\"]\n",
    "\n",
    "    for word in review.split(\" \"):\n",
    "        if word in VOCAB_IDX:\n",
    "            dict_feqs[sentiment][word] = dict_feqs[sentiment].get(word, 0) +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"story idx:\",VOCAB_IDX[\"story\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dict_feqs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mpositive good\u001b[39m\u001b[39m\"\u001b[39m, dict_feqs[\u001b[39m\"\u001b[39m\u001b[39mpositiv\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mgood\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mnegative good\u001b[39m\u001b[39m\"\u001b[39m, dict_feqs[\u001b[39m\"\u001b[39m\u001b[39mnegative\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mgood\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mnegative bad\u001b[39m\u001b[39m\"\u001b[39m, dict_feqs[\u001b[39m\"\u001b[39m\u001b[39mnegative\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mbad\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dict_feqs' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"positive good\", dict_feqs[\"positiv\"][\"good\"])\n",
    "print(\"negative good\", dict_feqs[\"negative\"][\"good\"])\n",
    "print(\"negative bad\", dict_feqs[\"negative\"][\"bad\"])\n",
    "print(\"positive good\", dict_feqs[\"positive\"][\"bad\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_postive = sum(dict_feqs[\"positive\"].values())\n",
    "\n",
    "word_table[\"positive\"] = [(dict_feqs[\"positive\"].get(w,0)+1) /\n",
    "                          (total_postive + len(VOCAB)) for w in word_table[\"word\"]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5238573367df39f7286bb46f9ff5f08f63a01a80960060ce41e3c79b190280fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
